{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC421 Assignment 3 - Part II Naive Bayes Classification (5 points) #\n",
    "### Author: George Tzanetakis \n",
    "\n",
    "This notebook is based on the supporting material for topics covered in **Chapter 13 Quantifying Uncertainty**and **Chapter 20 - Statistical Learning Method** from the book *Artificial Intelligence: A Modern Approach.* This part does NOT rely on the provided code so you can complete it just using basic Python. \n",
    "\n",
    "```\n",
    "Misunderstanding of probability may be the greatest of all impediments\n",
    "to scientific literacy.\n",
    "\n",
    "Gould, Stephen Jay\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "\n",
    "Text categorization is the task of assigning a given document to one of a fixed set of categories, on the basis of text it contains. Naive Bayes models are often used for this task. In these models, the query variable is\n",
    "the document category, and the effect variables are the presence/absence\n",
    "of each word in the language; the assumption is that words occur independently in documents within a given category (condititional independence), with frequencies determined by document category. Download the following file: http://www.cs.cornell.edu/People/pabo/movie-review-data/review_polarity.tar.gz containing a dataset that has been used for text mining consisting of movie reviews classified into negative and positive. You\n",
    "will see that there are two folders for the positivie and negative category and they each contain multiple text files with the reviews. You can find more information about the dataset at: \n",
    "http://www.cs.cornell.edu/People/pabo/movie-review-data/\n",
    "\n",
    "\n",
    "Our goal will be to build a simple Naive Bayes classifier for this dataset. More complicated approaches using term frequency and inverse document frequency weighting and many more words are possible but the basic concepts\n",
    "are the same. The goal is to understand the whole process so DO NOT use existing machine learning packages but rather build the classifier from scratch.\n",
    "\n",
    "Our feature vector representation for each text file will be simply a binary vector that shows which of the following words are present in the text file: Awful Bad Boring Dull Effective Enjoyable Great Hilarious. For example the text file cv996 11592.txt would be represented as (0, 0, 0, 0, 1, 0, 1, 0) because it contains Effective and Great but none of the other words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2A (Minimum) CSC421 -  (1 point, CSC581C - 0 points) \n",
    "\n",
    "Write code that parses the text files and calculates the probabilities for\n",
    "each dictionary word given the review polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative probs [0.099 0.503 0.166 0.09  0.046 0.053 0.282 0.048]\n",
      "positive probs [0.019 0.254 0.048 0.023 0.12  0.094 0.405 0.125]\n"
     ]
    }
   ],
   "source": [
    "dictionary = {\n",
    "    \"awful\": 0,\n",
    "    \"bad\": 1,\n",
    "    \"boring\": 2, \n",
    "    \"dull\": 3, \n",
    "    \"effective\": 4,\n",
    "    \"enjoyable\": 5,\n",
    "    \"great\": 6,\n",
    "    \"hilarious\": 7,\n",
    "}\n",
    "# YOUR CODE GOES HERE \n",
    "\n",
    "# Calculate negative review encodings\n",
    "negative_encodings = []\n",
    "positive_encodings = []\n",
    "\n",
    "negative_reviews = [\"reviews/neg/\" + f_name for f_name in os.listdir(\"reviews/neg/\")]\n",
    "positive_reviews = [\"reviews/pos/\" + f_name for f_name in os.listdir(\"reviews/pos/\")]\n",
    "\n",
    "# Loop through negative files\n",
    "for file in negative_reviews:\n",
    "    encoding = np.array([0] * 8)\n",
    "    for word in open(file).read().split(\" \"):\n",
    "        if word in dictionary: encoding[dictionary[word]] = 1\n",
    "    negative_encodings.append(encoding)\n",
    "\n",
    "# Loop through positive files\n",
    "for file in positive_reviews:\n",
    "    encoding = np.array([0] * 8)\n",
    "    for word in open(file).read().split(\" \"):\n",
    "        if word in dictionary: encoding[dictionary[word]] = 1\n",
    "    positive_encodings.append(encoding)\n",
    "\n",
    "    \n",
    "negative_encodings = np.array(negative_encodings)\n",
    "positive_encodings = np.array(positive_encodings)\n",
    "\n",
    "neg_probs = np.sum(negative_encodings, axis=0) / len(negative_reviews)\n",
    "pos_probs = np.sum(positive_encodings, axis=0) / len(positive_reviews)\n",
    "\n",
    "print(\"negative probs\",neg_probs)\n",
    "print(\"positive probs\",pos_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2B (Minimum) (CSC421 - 1 point, CSC581C - 0 point) \n",
    "\n",
    "\n",
    "Explain how the probability estimates for each dictionary word given the review polarity can be combined to form a Naive Bayes classifier. You can look up Bernoulli Bayes model for this simple model where only presence/absence of a word is modeled.\n",
    "\n",
    "Your answer should be a description of the process with equations and a specific example as markdown text NOT python code. You will write the code in the next questinon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## YOUR MARKDOWN TEXT GOES HERE \n",
    "The Naive bayes classifier will take in the encoding vector of a given text file. From this vector, it will calculate the probability of pos given ( x_1, x_2 ... x_8 ). With this vector the program will determine the probability of each class given that vector, i.e. P(C=pos|X) = P(x_1 | C=pos) * P(x_2 | C=pos) * ... P(x_8 | C=pos). This information can be retrieved from the models I defined above which are represented as\n",
    "```Python\n",
    "neg_probs=[0.099, 0.503, 0.166, 0.09,  0.046, 0.053, 0.282, 0.048]\n",
    "```\n",
    "The translation of notation is `neg_probs[i]` = P(x_i | C=neg)\n",
    "\n",
    "#### Example\n",
    "X = (1,0,1,1,0,0,0,1)\n",
    "\n",
    "P(C=pos|X) = pos_probs[0] * (1 - pos_probs[1]) * pos_probs[3] * ... * pos_probs[7]\n",
    "\n",
    "P(C=neg|X) = neg_probs[0] * (1 - neg_probs[1]) * neg_probs[3] * ... * neg_probs[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2C (Expected) 1 point \n",
    "\n",
    "Write Python code for classifying a particular test instance (in our case movie review) following a Bernolli Bayes approach. Your code should calculate the likelihood the review is positive given the correspondng conditional probabilities for each dictionary word as well as the likelihood the review is negative given the corresponding conditional probabilities for each dictionary word. Check that your code works by providing a few example cases of prediction. Your code should be written from \"scratch\" and only use numpy/scipy but not machine learning libraries like scikit-learn or tensorflow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[756. 244.]\n",
      " [411. 589.]]\n",
      "Accuracy: \n",
      " [[0.756 0.244]\n",
      " [0.411 0.589]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "def classify(X):\n",
    "    pos_score = 1\n",
    "    neg_score = 1\n",
    "    \n",
    "    # Compute class scores\n",
    "    for word,prob in zip(X,neg_probs):\n",
    "        if word == 1: neg_score *= prob\n",
    "        else: neg_score *= (1 - prob)            \n",
    "    for word,prob in zip(X,pos_probs):\n",
    "        if word == 1: pos_score *= prob\n",
    "        else: pos_score *= (1 - prob)\n",
    "    \n",
    "    # Return class with higher probability\n",
    "\n",
    "    return np.argmax([pos_score, neg_score])\n",
    "    \n",
    "    #Returns list of length 2, first position is positive class, second is negative\n",
    "    \n",
    "    return ret\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2D (Expected ) 1 point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the classification accuracy and confusion matrix that you would obtain using the whole data set for both training and testing. Do not use machine learning libraries like scikit-learn or tensorflow for this only the basic numpy/scipy stuff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[756. 244.]\n",
      " [411. 589.]]\n",
      "Accuracy: \n",
      " [[0.756 0.244]\n",
      " [0.411 0.589]]\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "confusion_matrix = np.zeros(shape=(2,2))\n",
    "\n",
    "for X in positive_encodings:\n",
    "    confusion_matrix[0][classify(X)] += 1\n",
    "    \n",
    "for X in negative_encodings:\n",
    "    confusion_matrix[1][classify(X)] += 1\n",
    "\n",
    "print(confusion_matrix)\n",
    "\n",
    "confusion_accuracy = confusion_matrix / len(negative_encodings)\n",
    "                                            \n",
    "print(\"Accuracy: \\n\",confusion_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2E (Advanced) 1 point \n",
    "\n",
    "One can consider the Naive Bayes classifier a generative model that can generate binary feature vectors using the associated probabilities from the training data. The idea is similar to how we do direct sampling in\n",
    "Bayesian Networks and depends on generating random number from a discrete distribution. Describe how you would generate random movie reviews consisting solely of the words from the dictionary using your model. Show 5 examples of randomly generated positive reviews and 5 examples of randomly generated negative reviews. Each example should consists of a subset of the words in the dictionary. Hint: use probabilities to generate both the presence and absence of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Reviews:\n",
      "['bad', 'boring', 'awful', 'great', 'bad', 'boring', 'awful', 'bad', 'great', 'bad']\n",
      "['hilarious', 'bad', 'enjoyable', 'boring', 'boring', 'bad', 'bad', 'bad', 'bad', 'effective']\n",
      "['hilarious', 'bad', 'great', 'bad', 'bad', 'bad', 'bad', 'bad', 'great', 'enjoyable']\n",
      "['awful', 'bad', 'awful', 'bad', 'bad', 'bad', 'bad', 'bad', 'bad', 'bad']\n",
      "['bad', 'bad', 'boring', 'great', 'great', 'hilarious', 'bad', 'effective', 'bad', 'dull']\n",
      "\n",
      "Positive Reviews:\n",
      "['enjoyable', 'hilarious', 'effective', 'great', 'great', 'effective', 'great', 'enjoyable', 'bad', 'great']\n",
      "['hilarious', 'dull', 'bad', 'great', 'enjoyable', 'great', 'enjoyable', 'bad', 'awful', 'enjoyable']\n",
      "['great', 'great', 'dull', 'effective', 'bad', 'bad', 'great', 'enjoyable', 'hilarious', 'great']\n",
      "['bad', 'hilarious', 'great', 'great', 'effective', 'great', 'enjoyable', 'bad', 'enjoyable', 'effective']\n",
      "['hilarious', 'hilarious', 'great', 'bad', 'hilarious', 'hilarious', 'great', 'great', 'great', 'great']\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE GOES HERE \n",
    "def make_review(probabilities):\n",
    "    terms = [\"awful\",\"bad\",\"boring\",\"dull\",\"effective\",\"enjoyable\",\"great\",\"hilarious\"]\n",
    "\n",
    "    rv = stats.rv_discrete(name = 'Review', \n",
    "                           values = (np.arange(len(terms)), probabilities / np.sum(probabilities)))\n",
    "    numeric_samples = rv.rvs(size=10)\n",
    "    mapped_samples = [terms[x] for x in numeric_samples]\n",
    "    return mapped_samples\n",
    "\n",
    "print(\"Negative Reviews:\")\n",
    "for _ in range(5):\n",
    "    print(make_review(neg_probs))\n",
    "print()\n",
    "print(\"Positive Reviews:\")\n",
    "for _ in range(5):\n",
    "    print(make_review(pos_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION 2F (ADVANCED) (CSC421 - 0 points, CSC581C - 2 points)\n",
    "\n",
    "Check the associated README file and see what convention is used for the 10-fold cross-validation. Calculate the classification accuracy and confusion matrix using the recommended 10-fold cross-validation. Again do NOT use \n",
    "ML libraries such as scikit-learn or tensorflow and just use numpy/scipy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
